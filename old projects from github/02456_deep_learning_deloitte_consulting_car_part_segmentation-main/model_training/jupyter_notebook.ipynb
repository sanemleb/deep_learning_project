{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_HKY1tCHTEh"
      },
      "outputs": [],
      "source": [
        "# Load packages\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from scipy.io import loadmat\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import array_to_img\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFwxzzvEb2aD",
        "outputId": "db6253d9-d792-4540-f43a-8d1bdab4cd9b"
      },
      "outputs": [],
      "source": [
        "# Mount (if using) google drive to the google colab jupyter notebook\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBxXd9GzHzIO"
      },
      "outputs": [],
      "source": [
        "# Define path here\n",
        "myPath = \"drive/MyDrive/Dataset_group_50/\"\n",
        "\n",
        "# Define parameters here\n",
        "IMAGE_SIZE = 256\n",
        "BATCH_SIZE = 4\n",
        "NUM_CLASSES = 9\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_SPLIT = 42\n",
        "LEARNING_RATE = 0.0001\n",
        "DATA_DIR = myPath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "G5OGAkHzJKJ9",
        "outputId": "8e8c3b5f-87e9-4114-80e0-57a173bc3e36"
      },
      "outputs": [],
      "source": [
        "# Test dataset is loaded correctly by displaying a image\n",
        "img = cv2.imread(myPath + \"data/Images/57_a.png\")\n",
        "cv2_imshow(img)\n",
        "# or\n",
        "# cv2.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZxYLGx5K3z3",
        "outputId": "9969ee55-2785-4c7d-a8b9-a6d4866bf8e0"
      },
      "outputs": [],
      "source": [
        "# Load training and valiation dataset\n",
        "X = sorted(glob(os.path.join(DATA_DIR, \"data/Images/*\")))\n",
        "y = sorted(glob(os.path.join(DATA_DIR, \"data/Category_ids/*\")))\n",
        "\n",
        "# Load test dataset\n",
        "test_images = sorted(glob(os.path.join(DATA_DIR, \"data/Test_data/Images/*\")))\n",
        "test_masks = sorted(glob(os.path.join(DATA_DIR, \"data/Test_data/Category_ids/*\")))\n",
        "\n",
        "# Add mirrored data\n",
        "X = X + (sorted(glob(os.path.join(DATA_DIR, \"mirrored/images/*\"))))\n",
        "y = y + (sorted(glob(os.path.join(DATA_DIR, \"mirrored/labels/*\"))))\n",
        "# Add gaussianblur data\n",
        "X = X + (sorted(glob(os.path.join(DATA_DIR, \"gaussianblur/images/*\"))))\n",
        "y = y + (sorted(glob(os.path.join(DATA_DIR, \"gaussianblur/labels/*\"))))\n",
        "# Add contrast-50 data\n",
        "X = X + (sorted(glob(os.path.join(DATA_DIR, \"contrast-50/images/*\"))))\n",
        "y = y + (sorted(glob(os.path.join(DATA_DIR, \"contrast-50/labels/*\"))))\n",
        "# Add contrast+50 data\n",
        "X = X + (sorted(glob(os.path.join(DATA_DIR, \"contrast+50/images/*\"))))\n",
        "y = y + (sorted(glob(os.path.join(DATA_DIR, \"contrast+50/labels/*\"))))\n",
        "# Add brightness-50 data\n",
        "X = X + (sorted(glob(os.path.join(DATA_DIR, \"brightness-50/images/*\"))))\n",
        "y = y + (sorted(glob(os.path.join(DATA_DIR, \"brightness-50/labels/*\"))))\n",
        "# Add brightness+50 data\n",
        "X = X + (sorted(glob(os.path.join(DATA_DIR, \"brightness+50/images/*\"))))\n",
        "y = y + (sorted(glob(os.path.join(DATA_DIR, \"brightness+50/labels/*\"))))\n",
        "# Add sharpen data\n",
        "X = X + (sorted(glob(os.path.join(DATA_DIR, \"sharpen/images/*\"))))\n",
        "y = y + (sorted(glob(os.path.join(DATA_DIR, \"sharpen/labels/*\"))))\n",
        "\n",
        "train_images, val_images, train_masks, val_masks = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_SPLIT)\n",
        "\n",
        "print(\"Number of training images: \" + str(len(train_images)))\n",
        "print(\"Number of training masks: \" + str(len(train_masks)))\n",
        "print(\"Number of validation images: \" + str(len(val_images)))\n",
        "print(\"Number of validation masks: \" + str(len(val_masks)))\n",
        "print(\"Number of test images: \" + str(len(test_images)))\n",
        "print(\"Number of test masks: \" + str(len(test_masks)))\n",
        "\n",
        "def read_image(image_path, mask=False):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    if mask:\n",
        "        image = tf.image.decode_png(image, channels=1)\n",
        "        image.set_shape([None, None, 1])\n",
        "        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
        "    else:\n",
        "        image = tf.image.decode_png(image, channels=3)\n",
        "        image.set_shape([None, None, 3])\n",
        "        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
        "    return image\n",
        "\n",
        "\n",
        "def load_data(image_list, mask_list):\n",
        "    image = read_image(image_list)\n",
        "    mask = read_image(mask_list, mask=True)\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "def data_generator(image_list, mask_list):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n",
        "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_dataset = data_generator(train_images, train_masks)\n",
        "val_dataset = data_generator(val_images, val_masks)\n",
        "test_dataset = data_generator(test_images, test_masks)\n",
        "\n",
        "print(\"Train Dataset:\", train_dataset)\n",
        "print(\"Val Dataset:\", val_dataset)\n",
        "print(\"Test Dataset:\", test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2eTC-vzldk2"
      },
      "outputs": [],
      "source": [
        "def convolution_block(\n",
        "    block_input,\n",
        "    num_filters=256,\n",
        "    kernel_size=3,\n",
        "    dilation_rate=1,\n",
        "    padding=\"same\",\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2D(\n",
        "        num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dilation_rate=dilation_rate,\n",
        "        padding=\"same\",\n",
        "        use_bias=use_bias,\n",
        "        kernel_initializer=keras.initializers.HeNormal(),\n",
        "    )(block_input)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def DilatedSpatialPyramidPooling(dspp_input):\n",
        "    dims = dspp_input.shape\n",
        "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
        "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
        "    out_pool = layers.UpSampling2D(\n",
        "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "\n",
        "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
        "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
        "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
        "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
        "    output = convolution_block(x, kernel_size=1)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpgqZ6DglgTK",
        "outputId": "c0a5ddae-5f31-4764-b6de-32b5cced9e28"
      },
      "outputs": [],
      "source": [
        "def DeeplabV3Plus(image_size, num_classes):\n",
        "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
        "    resnet50 = keras.applications.ResNet50(\n",
        "        weights=\"imagenet\", include_top=False, input_tensor=model_input\n",
        "    )\n",
        "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
        "    x = DilatedSpatialPyramidPooling(x)\n",
        "\n",
        "    input_a = layers.UpSampling2D(\n",
        "        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
        "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
        "    x = convolution_block(x)\n",
        "    x = convolution_block(x)  \n",
        "    x = layers.UpSampling2D(\n",
        "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n",
        "    return keras.Model(inputs=model_input, outputs=model_output)\n",
        "\n",
        "\n",
        "model = DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E4sV_IOAlsHI",
        "outputId": "9019d1b8-70dc-4e16-91a5-d87e684300e1"
      },
      "outputs": [],
      "source": [
        "# Define loss\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# Compile model with choosen optimizer \n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss=loss,\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "# Train model\n",
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=25)\n",
        "\n",
        "# Create plots\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.title(\"Training Loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"accuracy\"])\n",
        "plt.title(\"Training Accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.title(\"Validation Loss\")\n",
        "plt.ylabel(\"val_loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.ylabel(\"val_accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_9VpOG6opsB"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "path = \"drive/MyDrive/carseg_data/\" + datetime.now().strftime(\"%m-%d-%Y_%H-%M-%S\") + \"_model\"\n",
        "model.save(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUbGvrv6sEiD",
        "outputId": "beda1f4c-e889-4feb-f417-ba391f761fb3"
      },
      "outputs": [],
      "source": [
        "# To create segmentation with overlays download color palette\n",
        "!gdown https://drive.google.com/uc?id=1B9A9UCJYMwTL4oBEo4RZfbMZMaZhKJaz\n",
        "!unzip -q instance-level-human-parsing.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RPBVg8Wr1ud"
      },
      "outputs": [],
      "source": [
        "# Loading the Colormap\n",
        "# colormap = loadmat(\n",
        "#     \"./instance-level_human_parsing/instance-level_human_parsing/human_colormap.mat\"\n",
        "# )[\"colormap\"]\n",
        "# colormap = colormap * 100\n",
        "# colormap = colormap.astype(np.uint8)\n",
        "\n",
        "def infer(model, image_tensor):\n",
        "    predictions = model.predict(np.expand_dims((image_tensor), axis=0))\n",
        "    predictions = np.squeeze(predictions)\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def decode_segmentation_masks(mask, colormap, n_classes):\n",
        "    r = np.zeros_like(mask).astype(np.uint8)\n",
        "    g = np.zeros_like(mask).astype(np.uint8)\n",
        "    b = np.zeros_like(mask).astype(np.uint8)\n",
        "    for l in range(0, n_classes):\n",
        "        idx = mask == l\n",
        "        r[idx] = colormap[l, 0]\n",
        "        g[idx] = colormap[l, 1]\n",
        "        b[idx] = colormap[l, 2]\n",
        "    rgb = np.stack([r, g, b], axis=2)\n",
        "    return rgb\n",
        "\n",
        "\n",
        "def get_overlay(image, colored_mask):\n",
        "    image = tf.keras.preprocessing.image.array_to_img(image)\n",
        "    image = np.array(image).astype(np.uint8)\n",
        "    overlay = cv2.addWeighted(image, 0.35, colored_mask, 0.65, 0)\n",
        "    return overlay\n",
        "\n",
        "\n",
        "def plot_samples_matplotlib(display_list, figsize=(5, 3)):\n",
        "    _, axes = plt.subplots(nrows=1, ncols=len(display_list), figsize=figsize)\n",
        "    for i in range(len(display_list)):\n",
        "        if display_list[i].shape[-1] == 3:\n",
        "            axes[i].imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
        "        else:\n",
        "            axes[i].imshow(display_list[i])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_predictions(images_list, colormap, model):\n",
        "    for image_file in images_list:\n",
        "        image_tensor = read_image(image_file)\n",
        "        prediction_mask = infer(image_tensor=image_tensor, model=model)\n",
        "        prediction_colormap = decode_segmentation_masks(prediction_mask, colormap, 9)\n",
        "        overlay = get_overlay(image_tensor, prediction_colormap)\n",
        "        plot_samples_matplotlib(\n",
        "            [image_tensor, overlay, prediction_colormap], figsize=(18, 14)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpqM2wLTskY_"
      },
      "outputs": [],
      "source": [
        "# Plot 20 predections using images from the test set\n",
        "plot_predictions(test_images[:20], colormap, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XySUMAZxjTE",
        "outputId": "9e0676e6-96c3-4064-c4c9-00ae3cd7b5ed"
      },
      "outputs": [],
      "source": [
        "# Evaulate model using the inbuilt method.\n",
        "model.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yuK7uGHdCuf",
        "outputId": "038350dd-bdec-4d9f-8c3c-46a398c746f0"
      },
      "outputs": [],
      "source": [
        "# Evaluate using DICE score\n",
        "DICE_SCORES = []\n",
        "\n",
        "for k in range(len(test_images)):\n",
        "  image_tensor = read_image(test_images[k])\n",
        "  label_tensor = cv2.imread(test_masks[k])\n",
        "  label_tensor = label_tensor[:,:,0].flatten()\n",
        "  prediction_mask = infer(image_tensor=image_tensor, model=model)\n",
        "  prediction_mask = prediction_mask.flatten()\n",
        "\n",
        "  TP = [0]*9\n",
        "  NTP = [0]*9\n",
        "  DICE = []\n",
        "  for i in range(len(prediction_mask)):\n",
        "      if prediction_mask[i] == label_tensor[i]:\n",
        "          TP[prediction_mask[i]] = TP[prediction_mask[i]]+1\n",
        "\n",
        "      else:\n",
        "          NTP[prediction_mask[i]] = NTP[prediction_mask[i]]+1\n",
        "\n",
        "  for j in range(len(TP)):\n",
        "    smooth = 1000\n",
        "    DICE.append((2*TP[j]+smooth)/(2*TP[j] + NTP[j]+smooth))\n",
        "  DICE_SCORES.append(DICE)\n",
        "\n",
        "DICE_MATRIX = np.array(DICE_SCORES)\n",
        "\n",
        "print(\"Background: \", \"%.2f\" % (DICE_MATRIX.mean(axis=0)[0]*100),\"%\")\n",
        "print(\"Front Door: \", \"%.2f\" % (DICE_MATRIX.mean(axis=0)[1]*100),\"%\")\n",
        "print(\"Back Door: \", \"%.2f\" % (DICE_MATRIX.mean(axis=0)[2]*100),\"%\")\n",
        "print(\"Front Fender: \", \"%.2f\" % (DICE_MATRIX.mean(axis=0)[3]*100),\"%\")\n",
        "print(\"Body: \", \"%.2f\" % (DICE_MATRIX.mean(axis=0)[4]*100),\"%\")\n",
        "print(\"Front Bumper: \", \"%.2f\" % (DICE_MATRIX.mean(axis=0)[5]*100),\"%\")\n",
        "print(\"Hood: \", \"%.2f\" % (DICE_MATRIX.mean(axis=0)[6]*100),\"%\")\n",
        "print(\"Rear Bumper: \", \"%.2f\" % (DICE_MATRIX.mean(axis=0)[7]*100),\"%\")\n",
        "print(\"Trunk: \", \"%.2f\" % (DICE_MATRIX.mean(axis=0)[8]*100),\"%\")\n",
        "print(\"Combined DICE score: \",\"%.2f\" % (np.mean(DICE_MATRIX.mean(axis=0))*100),\"%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSoxBLTlQXMe"
      },
      "outputs": [],
      "source": [
        "# Save Dice score\n",
        "np.save('drive/MyDrive/carseg_data/Gabriel_dataset_3_0_history_test.npy', history.history) "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
